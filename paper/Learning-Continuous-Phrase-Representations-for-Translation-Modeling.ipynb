{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Continuous Phrase Representations for Translation Modeling\n",
    "\n",
    "    Jianfeng Gao,Xiaodong He,Wen-tau Yih,Li Deng\n",
    "    2013\n",
    "\n",
    "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/nn4smt.acl_.v9.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "1. è®­ç»ƒé›†ï¼šå¥å­pairï¼ˆæºï¼Œç›®æ ‡ï¼‰ï¼Œå³ï¼ˆfï¼Œeï¼‰\n",
    "1. æ— è®ºæ˜¯sourceè¿˜æ˜¯target\n",
    "    1. æŠŠbag-of-words representation\u001bè½¬æˆ low-dimensional continuous space\n",
    "    1. å†æ±‚ï¼ˆfï¼Œeï¼‰ç›¸ä¼¼åº¦\n",
    "    1. ç”¨softmaxè®¡ç®—ğ‘ƒ(ğ¸|ğ¹ğ‘–) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The phrase translation model, also known as the phrase table, is one of the core components of phrase-based `statistical machine translation` (`SMT`) systems.The most common method of constructing the phrase table takes a two-phase approach.\n",
    "\n",
    "- First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data.\n",
    "- The second phase, which is the focus of this paper, is parameter estimation where each phrase pair is assigned with some scores that are estimated based on counting these phrases or their words using the same word-aligned training data.\n",
    "\n",
    "This paper revisits the problem of scoring a phrase translation pair by developing a `Continuous-space Phrase Translation Model` (`CPTM`).\n",
    "\n",
    "- First, we represent each phrase as a bag-of-words vector, called `word vector` henceforth.\n",
    "- We then project the word vector, in either the source language or the target language, into a respective continuous feature vector in a common low-dimensional space that is language independent. The projection is performed by a multi-layer neural network. The projected feature vector forms the `continuous representation` of a phrase.\n",
    "- Finally, the translation score of a source-target phrase pair is computed by the distance between their feature vectors.\n",
    "\n",
    "## The Log-Linear Model for SMT\n",
    "Phrase-based SMT is based on a log-linear model which requires learning a mapping between input ğ¹ âˆˆ â„± to output ğ¸ âˆˆ â„°. We are given\n",
    "\n",
    "- Training samples (ğ¹ğ‘–, ğ¸ğ‘–) for ğ‘– = 1 â€¦ ğ‘, where each source sentence ğ¹ğ‘– is paired with a reference translation in target language ğ¸ğ‘–;\n",
    "- A procedure GEN to generate a list of N-best candidates GEN(ğ¹ğ‘–) for an input ğ¹ğ‘–, each ğ¸ âˆˆ GEN(ğ¹ğ‘–) is labeled by the sentence-level BLEU score (He and Deng 2012), denoted by sBleu(ğ¸ğ‘–,ğ¸) , which measures the quality of ğ¸ with respect to its reference translation ğ¸ğ‘–;\n",
    "- A vector of features $h \\in \\mathbb{R}^M$ that maps each(ğ¹ğ‘–, ğ¸) to a vector of feature values;\n",
    "- A parameter vector $\\lambda \\in \\mathbb{R}^M$, which assigns a real-valued weight to each feature. \n",
    "\n",
    "$$E^*=argmax_{(E,A) \\in GEN(F_i)} \\lambda^Th(F_i,E,A)\\ (1)$$\n",
    "\n",
    "which states that given ğ›Œ and ğ¡, argmax returns the highest scoring translation $E^*$, maximizing over correspondences ğ´.\n",
    "\n",
    "## A Continuous-Space Phrase Translation Model (CPTM)\n",
    "We start with a bag-of-words representation of a phrase $x \\in \\mathbb{R}^d$, where ğ± is a word vector and ğ‘‘ is the size of the vocabulary consisting of words in both source and target languages, which is set to 200K in our experiments. We then learn to project\n",
    "ğ± to a low-dimensional continuous space $\\mathbb{R}^k$:\n",
    "\n",
    "$$\\phi(x):\\mathbb{R}^d \\to \\mathbb{R}^k$$\n",
    "\n",
    "Let ğ–1 be the projection matrix from the input layer to the hidden layer and ğ–2 the projection matrix from the hidden layer to the output layer, we have\n",
    "\n",
    "$$y \\equiv \\phi(x) = tanh(W_2^T(tanh(W_1^T x)))\\ (2)$$\n",
    "\n",
    "$$score(f,e) \\equiv sim_{\\theta}(x_f,x_e)=y_f^T y_e\\ (3)$$\n",
    "\n",
    "![1](http://ou8qjsj0m.bkt.clouddn.com//17-8-8/90077512.jpg)\n",
    "\n",
    "![2](http://ou8qjsj0m.bkt.clouddn.com//17-8-8/7292310.jpg)\n",
    "\n",
    "The CPTM of (2) and (3) can be incorporated into the log-linear model for SMT (1) by introducing a new feature â„ğ‘€+1 and a new feature weight ğœ†ğ‘€+1. The new feature is defined as\n",
    "\n",
    "$$h_{M+1}(F_i,E,A)=\\sum_{(f,e) \\in A} sim_{\\theta}(x_f,x_e)\\ (4)$$\n",
    "\n",
    "## Training CPTM\n",
    "We define the loss function â„’(ğ›‰) as the negative of the N-best list based expected BLEU, denoted by xBleu(ğ›‰)\n",
    "\n",
    "$$xBleu(\\theta)=\\sum_{E \\in GEN(F_i)} P(E|F_i)sBleu(E_i,E)\\ (5)$$\n",
    "\n",
    "where sBleu(ğ¸ğ‘–, ğ¸) is the sentence-level BLEU score, and ğ‘ƒ(ğ¸|ğ¹ğ‘–) is the translation probability from ğ¹ğ‘– to ğ¸ computed using softmax as \n",
    "\n",
    "$$P(E|F_i)=\\frac{exp(\\gamma\\lambda^T h(F_i,E,A))}{\\sum_{E' \\in GEN(F_i)}exp(\\gamma\\lambda^T h(F_i,E',A))}\\ (6)$$\n",
    "\n",
    "where $\\lambda^T h$ is the log-linear model of (1), which also includes the feature derived from the CPTM as defined by (4), and ğ›¾ is a tuned smoothing factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
