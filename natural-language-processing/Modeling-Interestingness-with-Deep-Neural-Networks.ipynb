{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Interestingness with Deep Neural Networks\n",
    "\n",
    "    Jianfeng Gao, Patrick Pantel, Michael Gamon, Xiaodong He, Li Deng\n",
    "    2014\n",
    "\n",
    "https://www.microsoft.com/en-us/research/wp-content/uploads/2014/10/604_Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "ä¸¤ä¸ªåŠŸèƒ½ï¼š\n",
    "\n",
    "1. è‡ªåŠ¨æ¨èæ„Ÿå…´è¶£çš„å®ä½“ï¼ˆå³å½“å‰æ–‡ç« çš„å…³é”®å­—ï¼‰ï¼Œä¾¿äºä½œä¸ºåç»­æ–‡ç« æ¨èçš„ä¾æ®\n",
    "1. æ¨èæ–°çš„å…´è¶£æ–‡ç« ï¼Œå…³äºä¸Šè¿°å®ä½“çš„\n",
    "\n",
    "æ¨¡å‹æ¶æ„ï¼š\n",
    "\n",
    "1. Input Layer x\n",
    "    1. æ¯ä¸ªè¾“å…¥wordæ˜¯ä¸€ä¸ª one-hot vectorï¼Œå®éªŒæ‰€ç”¨vocabulary 150K\n",
    "    1. æŠŠæ¯ä¸ªwæ‹†åˆ†æˆtri-letter vectorï¼Œ30K\n",
    "    1. build x\n",
    "1. Convolutional Layer u\n",
    "    1. å¯¹æ¯ä¸ªw extracts local features\n",
    "1. Max-pooling Layer v\n",
    "    1. å¾—åˆ°globalçš„feature\n",
    "1. Fully-Connected Layers h and y\n",
    "\n",
    "è®­ç»ƒDSSMï¼š\n",
    "\n",
    "å¯¹äºinterestingnesséœ€è¦æ ‡æ³¨ç›®æ ‡æ–‡æ¡£ï¼ˆå¯ç›‘ç£ï¼‰\n",
    "\n",
    "ä½¿ç”¨ DSSMï¼š\n",
    "\n",
    "1. ä½œä¸ºç‰¹å¾\n",
    "1. ç›´æ¥ä½œä¸ºinterestingnessçš„é¢„æµ‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- `Automatic Highlighting`. In this task we want a recommendation system to automatically discover the entities (e.g., a person, location, organization etc.) that interest a user when reading a document and to highlight the corresponding text spans, referred to as `keywords` afterwards.\n",
    "- `Contextual entity search`. After identifying the keywords that represent the entities of interest to the user, we also want the system to recommend new, interesting documents by searching the Web for supplementary information about these entities. \n",
    "\n",
    "However, their model is designed to represent the `relevance` between queries and documents, which differs from the notion of `interestingness` between documents studied in this paper. It is often the case that a user is interested in a document because it provides supplementary information about the entities or concepts she encounters when reading another document although the overall contents of the second documents is not highly relevant. \n",
    "\n",
    "To better model interestingness,\n",
    "\n",
    "1. DSSM treats a document as a sequence of words and tries to discover prominent keywords. These keywords represent the entities or concepts that might interest users, via the convolutional and max-pooling layers. The DSSM then forms the high-level semantic representation of the whole document based on these keywords.\n",
    "1. We feed the features derived from the semantic representations of documents to a ranker which is trained in a supervised manner.\n",
    "\n",
    "## The Notion of Interestingness\n",
    "Let ğ· be the set of all documents, we formally define the `interestingness` modeling task as learning the mapping function:\n",
    "\n",
    "$\\sigma:D \\times D \\to \\mathbb{R}^+$\n",
    "\n",
    "where the function ğœ(ğ‘ , ğ‘¡) is the quantified degree of interest that the user has in the target document ğ‘¡ âˆˆ ğ· after or while reading the source document ğ‘  âˆˆ ğ·.\n",
    "\n",
    "## A Deep Semantic Similarity Model(DSSM)\n",
    "### Network Architecture\n",
    "![1](http://ou8qjsj0m.bkt.clouddn.com//17-8-6/13679357.jpg)\n",
    "\n",
    "#### Input Layer x\n",
    "1. convert each word in d to a word vector\n",
    "    1. represent w by a one-hot vector using a vocabulary that contains N high frequent words(N = 150K in this study)\n",
    "    1.  map w to a separate tri-letter vector. Consider the word â€œ#dog#â€, where # is a word boundary symbol. The nonzero elements in its triletter vector are â€œ#doâ€, â€œdogâ€, and â€œog#â€. \n",
    "1. build x by concatenating these word vectors\n",
    "\n",
    "Although the number of unique English words on the Web is extremely large, the total number of distinct triletters in English is limited.\n",
    "\n",
    "#### Convolutional Layer u\n",
    "A convolutional layer extracts local features around each word $w_i$\n",
    "in a word sequence of length I as follows. \n",
    "\n",
    "1. generate a contextual vector $c_i$ by concatenating the word vectors of $w_i$ and its surrounding words defined by a window (the window size is set to 3 in this paper).\n",
    "1. we generate for each word a local feature vector $u_i$ using a tanh activation function and a linear projection matrix $W_c$, which is the same across all windows ğ‘– in the word sequence, as:\n",
    "\n",
    "$u_i=tanh(W_c^T c_i), where i=1,\\cdots,I$ (1)\n",
    "\n",
    "#### Max-pooling Layer v\n",
    "$v(j)=max_{i=1,\\cdots,I} \\{u_i(j)\\}$ (2)\n",
    "\n",
    "That convolutional and max-pooling layers are able to discover prominent keywords of a document can be demonstrated using the procedure in Figure 2\n",
    "\n",
    "![2](http://ou8qjsj0m.bkt.clouddn.com//17-8-6/45479331.jpg)\n",
    "\n",
    "1. the convolutional layer of (1) generates for each word in a 5-word document a 4-dimensional local feature vector, which represents a distribution of four `topics`\n",
    "    1. the most prominent topic of $w_2$ within its three word context window is the first topic, denoted by $u_2(1)$\n",
    "    1. the most prominent topic of $w_5$ is $u_5(3)$\n",
    "1.  use max-pooling of (2) to form a global feature vector, which represents the topic distribution of the whole document\n",
    "    1.  v(1) and v(3) are two prominent topics\n",
    "    1.  for each prominent topic, we trace back to the local feature vector that survives max-pooling:$v(1)=max_{i=1,\\cdots,5}\\{u_i(1)\\}=u_2(1),v(3)=max_{i=1,\\cdots,5}\\{u_i(3)\\}=u_5(3)$\n",
    "1. label the corresponding words of these local feature vectors, $w_2$ and $w_5$, as keywords of the document\n",
    "\n",
    "Figure 3 presents a sample of document snippets and their keywords detected by the DSSM according to the procedure elaborated in Figure 2.\n",
    "\n",
    "![3](http://ou8qjsj0m.bkt.clouddn.com//17-8-6/59818700.jpg)\n",
    "\n",
    "#### Fully-Connected Layers h and y\n",
    "$h=tanh(W_1^T v)$ (3)\n",
    "\n",
    "$y=tanh(W_2^T h)$ (4)\n",
    "\n",
    "where $W_1$ and $W_2$ are learned linear projection matrices.\n",
    "\n",
    "### Training the DSSM\n",
    "To optimize the parameters of the DSSM of Figure1, i.e., ğ›‰ = {ğ–ğ‘\n",
    ",ğ–1,ğ–2}, we use a pair-wise rank loss as objective.\n",
    "\n",
    "Consider a source document ğ‘  and two candidate target documents ğ‘¡1 and ğ‘¡2, where ğ‘¡1 is more interesting than ğ‘¡2 to a user when reading ğ‘ . We construct two pairs of documents (ğ‘ ,ğ‘¡1) and (ğ‘ ,ğ‘¡2), where the former is preferred and should have a higher interestingness score. Let âˆ† be the difference of their interestingness scores: âˆ† = ğœ(ğ‘ ,ğ‘¡1) âˆ’ ğœ(ğ‘ ,ğ‘¡2) , where ğœ is the interestingness score, computed as the cosine similarity:\n",
    "\n",
    "$\\sigma(s,t) \\equiv sim_{\\theta}(s,t) = \\frac{y_s^T y_t}{\\lVert y_s \\rVert \\lVert y_t \\rVert}$ (5)\n",
    "\n",
    "where ğ²ğ‘  and ğ²ğ‘¡ are the feature vectors of ğ‘  and ğ‘¡,respectively, which are generated using the DSSM, parameterized by ğ›‰.  Intuitively, we want to learn ğ›‰ to maximize âˆ†. \n",
    "\n",
    "We use the following logistic loss over âˆ† ,which can be shown to upper bound the pairwise accuracy:\n",
    "\n",
    "$\\mathbb{L}(\\Delta;\\theta)=log(1+exp(-\\gamma\\Delta))$ (6)\n",
    "\n",
    "### Using the DSSM\n",
    "1. we use the DSSM as a feature generator. The output layer of the DSSM can be seen as a set of semantic features, which can be incorporated in a boosted tree based ranker\n",
    "1. we use the DSSM as a direct implementation of the interestingness function $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
